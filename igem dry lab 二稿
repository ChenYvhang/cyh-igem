# -*- coding: utf-8 -*-
# ====================== 1. 环境配置 ======================
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # 屏蔽TensorFlow信息性日志
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Layer, Input, Conv1D, LSTM, Dense, Dropout, concatenate, RandomTranslation
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import regularizers
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import matplotlib.pyplot as plt
import logging

# 配置日志
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("HMPI")

# ====================== 2. 混合注意力层（兼容性优化） ======================
class HybridAttention(Layer):
    """跨版本兼容的混合注意力机制"""
    def __init__(self, ratio=8, **kwargs):
        super().__init__(**kwargs)
        self.ratio = ratio
        self.channel_axis = -1

    def build(self, input_shape):
        channel = input_shape[self.channel_axis]
        
        # 通道注意力
        self.channel_fc = tf.keras.Sequential([
            Dense(channel//self.ratio, activation='relu'),
            Dense(channel)
        ])
        
        # 空间注意力
        self.spatial_conv = Conv1D(1, 7, padding='same', activation='sigmoid')
        
        super().build(input_shape)

    def call(self, x):
        # 通道注意力
        channel_avg = tf.reduce_mean(x, axis=1, keepdims=True)
        channel_max = tf.reduce_max(x, axis=1, keepdims=True)
        channel = concatenate([channel_avg, channel_max], axis=self.channel_axis)
        channel_weights = tf.nn.sigmoid(self.channel_fc(channel))
        
        # 空间注意力
        spatial_weights = self.spatial_conv(x)
        
        # 特征融合
        return x * (channel_weights + spatial_weights)

# ====================== 3. 数据加载与增强 ======================
class DataLoader:
    """支持离线/在线模式的数据加载器"""
    def __init__(self, data_path=None):
        self.data_path = data_path
        
    def load(self):
        """优先尝试本地加载"""
        try:
            if self.data_path and os.path.exists(self.data_path):
                return self._load_local()
            return self._load_online()
        except Exception as e:
            logger.error(f"数据加载失败: {str(e)}")
            exit()

    def _load_online(self):
        from datasets import load_dataset
        logger.info("正在从HuggingFace下载数据集...")
        dataset = load_dataset('dna_core_promoter')
        return dataset['train'].to_pandas()

    def _load_local(self):
        import pandas as pd
        logger.info(f"正在从本地加载数据: {self.data_path}")
        return pd.read_csv(self.data_path)

# ====================== 4. HMPI模型构建 ======================
class HMPIModel:
    """支持动态调整的模型构建类"""
    def __init__(self, input_shape=(100,4), filters=32, lstm_units=64):
        self.input_shape = input_shape
        self.filters = filters
        self.lstm_units = lstm_units
        
    def build(self):
        inputs = Input(shape=self.input_shape)
        
        # 数据增强层
        x = RandomTranslation(height_factor=0.05, fill_mode='constant')(inputs)
        
        # 多尺度卷积
        conv_large = Conv1D(self.filters, 9, padding='same', activation='relu')(x)
        conv_medium = Conv1D(self.filters, 5, padding='same', activation='relu')(x)
        conv_small = Conv1D(self.filters, 3, padding='same', activation='relu')(x)
        merged_conv = concatenate([conv_large, conv_medium, conv_small])
        x = tf.keras.layers.MaxPooling1D(2)(merged_conv)
        
        # LSTM时序处理
        x = LSTM(self.lstm_units, return_sequences=True)(x)
        x = HybridAttention()(x)
        
        # 深度监督分支
        mid_output = GlobalAveragePooling1D()(x)
        mid_output = Dense(1, activation='sigmoid', name='mid_output')(mid_output)
        
        # 最终输出
        final_output = Dense(64, activation='relu',
                            kernel_regularizer=regularizers.l2(0.01))(x)
        final_output = Dropout(0.5)(final_output)
        final_output = Dense(1, activation='sigmoid', name='final_output')(final_output)
        
        model = Model(inputs=inputs, outputs=[final_output, mid_output])
        return model

# ====================== 5. 可视化回调 ======================
class AttentionVisualizer(tf.keras.callbacks.Callback):
    """注意力权重可视化回调"""
    def __init__(self, sample_data, output_dir='attention_maps'):
        super().__init__()
        self.sample_data = sample_data
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)
        
    def on_epoch_end(self, epoch, logs=None):
        try:
            attention_model = Model(
                inputs=self.model.input,
                outputs=self.model.get_layer('hybrid_attention').output
            )
            att_weights = attention_model.predict(self.sample_data)
            
            plt.figure(figsize=(10, 4))
            plt.imshow(att_weights[0].T, aspect='auto', cmap='viridis')
            plt.colorbar()
            plt.title(f"Attention Weights - Epoch {epoch+1}")
            plt.savefig(f'{self.output_dir}/epoch_{epoch+1}.png')
            plt.close()
        except Exception as e:
            logger.warning(f"注意力可视化失败: {str(e)}")

# ====================== 6. 主程序 ======================
def main():
    # 配置参数
    config = {
        'data_path': None,  # 本地数据路径（可选）
        'max_length': 100,
        'batch_size': 32,    # 根据GPU内存调整
        'epochs': 20,
        'output_dir': 'results'
    }
    
    # 初始化环境
    os.makedirs(config['output_dir'], exist_ok=True)
    logger.info("实验配置：\n" + "\n".join(f"{k}: {v}" for k,v in config.items()))
    
    # 数据加载与预处理
    loader = DataLoader(config['data_path'])
    df = loader.load()
    
    # 数据预处理
    X = np.zeros((len(df), config['max_length'], 4))
    for i, seq in enumerate(df['sequence']):
        for j in range(min(len(seq), config['max_length'])):
            base = seq[j].upper()
            if base == 'A': X[i,j,0] = 1
            elif base == 'T': X[i,j,1] = 1
            elif base == 'C': X[i,j,2] = 1
            elif base == 'G': X[i,j,3] = 1
    
    y = df['label'].values.astype(np.float32)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, stratify=y, random_state=42)
    
    # 构建模型
    model_builder = HMPIModel(filters=16, lstm_units=32)  # 降低复杂度适应本地运行
    model = model_builder.build()
    
    # 编译模型
    model.compile(
        optimizer=Adam(learning_rate=0.001),
        loss={'final_output': 'binary_crossentropy', 'mid_output': 'binary_crossentropy'},
        loss_weights={'final_output': 0.7, 'mid_output': 0.3},
        metrics={'final_output': ['accuracy']}
    )
    
    # 训练配置
    callbacks = [
        tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),
        AttentionVisualizer(sample_data=X_test[:1])  # 可视化样例
    ]
    
    # 动态批处理（防止OOM）
    try:
        history = model.fit(
            X_train, {'final_output': y_train, 'mid_output': y_train},
            batch_size=config['batch_size'],
            epochs=config['epochs'],
            validation_split=0.2,
            callbacks=callbacks,
            verbose=1
        )
    except tf.errors.ResourceExhaustedError:
        logger.warning("检测到内存不足，自动减小批处理大小...")
        config['batch_size'] = 16
        history = model.fit(
            X_train, {'final_output': y_train, 'mid_output': y_train},
            batch_size=config['batch_size'],
            epochs=config['epochs'],
            validation_split=0.2,
            callbacks=callbacks,
            verbose=1
        )
    
    # 评估与保存
    logger.info("测试集评估结果：")
    y_pred, _ = model.predict(X_test, batch_size=config['batch_size'])
    print(classification_report(y_test, (y_pred > 0.5).astype(int)))
    
    # 保存模型
    model.save(os.path.join(config['output_dir'], 'final_model.h5'))
    logger.info(f"模型已保存至 {config['output_dir']}")

if __name__ == "__main__":
    # 设置TensorFlow日志级别
    tf.get_logger().setLevel('ERROR')
    # 验证文件编码
    assert __file__.endswith('.py'), "请确保文件扩展名为.py"
    main()
